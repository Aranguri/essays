{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things\n",
    "http://www.j-paine.org/make_category_theory_intuitive.html\n",
    "what if we have concepts and transitions and properties?\n",
    "\n",
    "## Introdc\n",
    "With a good representation, most machine learning problems can be solved by simple algorithms (like a neural net or k-nearest neighbors.) So, solving most machine learning problems is equivalent to finding a good representation of the data. \n",
    "\n",
    "## Human: How does the brain represent information?\n",
    "{does the brain map all the sensory input to the same place?}\n",
    "\n",
    "## What makes a good representation?\n",
    "Concepts are interrelated. And they can be more interrelated or less interrelated. The concept of a tree is related to that of grass, but it's pretty unrelated to the concept of a computer. So, we want a representation that captures these relationships.\n",
    "\n",
    "Also, we want a representation that captures the composability of concepts. We could have a function that takes two concepts and returns what both concepts mean (eg we have the concept of a \"tree\" and the concept of \"burned\", then we would want to produce \"burned tree.\") A more complex approach is that the operation we apply between the two concepts could also be a concept (or at least it could be more than just one option.)\n",
    "\n",
    "We also want a shared space. We want everything we know to be in the same space. This has two reasons. First, we want to compare things from different types of input (eg \"is what you are seeing your keyboard?\" you are comparing an image to the word \"keyboard\" without problems.) Second, we want to extend the knowledge from one field to the other, that is, we want to use analogies. \n",
    "\n",
    "Analogies are particularly interesting, because they are related to generalizing. An analogy is a way to extend knowledge from one field to the other. \n",
    "\n",
    "So, let's say we start with a empty learning system. We want this system to be as good as possible in mapping input with outputs. We are going to focus on the representation used along the way. The first step is when the learning system receives the input. We don't want all the incoming information to be kept all the time. In particular, we are trying to focus on one thing. Why is this so? The thing is that the information we receive tends to be redundant and most of it useless. This happens because knowing what part of the information is useful could have similar complexity to solving the task. Also, the features we care about depend on the task. \n",
    "\n",
    "An interesting problem that arises is that knowing what to remove depends on how well we removed information. Say we are a self-driving car and we carelessly remove the signal light that was in red. If we have no signal light, then we will focus our attention on looking in the front to check whether there was a car or not. \n",
    "\n",
    "Initially, it seems that the data can't get worse if we use PCA to decorrelate it. \n",
    "\n",
    "It's interesting to think about how humans do it. I think it can be abstracted in this way: we clearly don't receive the correct input (supervised learning.) Instead, the pleasure decreases or increases, and we have to guess what was the underlying cause of the generated pleasure. \n",
    "\n",
    "One way to do this is by having a model of the reality. In this model, we have complex causal relationship between input and outputs. \n",
    "\n",
    "One question to ask is if we represent concepts in the brain as discrete things or it's a continuum. It would be very interesting if it's a continuum, because we should have apple in one extremum book in other extremum and in the middle we should have the natural interpolation between apple and book. I can't retrieve from my brain an image of a apple-book.\n",
    "\n",
    "This doesn't imply that an apple-book doesn't exist in my brain. Indeed, maybe it exists but there is a mechanism that prevents this merging of concepts. Pragmatically, it makes sense to have this mechanism: we are more efficient. Reality is discrete. We have twenty fingers, one moon. There isn't a moon-finger anywhere in the world. Maybe the way we represent things in our brain is more like a bunch of pencils organized by color. You can't exactly point out where the green ends and where the yellow starts. But if we remove ourselves from the intersection between green and yellow, we can easily say that in some part, it's mostly yellow and in the other part is mostly green. \n",
    "\n",
    "This structure allows us to use derivatives.  \n",
    "\n",
    "Let's think about concepts in a discrete way, even if the way to represent it is something continuous. \n",
    "\n",
    "Question: is a neural net a model of the reality? can it be the structure for that goal?\n",
    "\n",
    "Modularity is important\n",
    "\n",
    "Also, I want to prove (or prove false) the equivalence between neural networks and reality model. \n",
    "\n",
    "Also, if neural nets \\iff reality model, then it would be interesting to make a neural net that drops some part of the input to learn causalities. In particular, it would be interesting to build neural nets that recognize causalities. \n",
    "\n",
    "Also, consider the connectedness between thoughts.\n",
    "\n",
    "## Project\n",
    "As I learn things, I want to apply them. Also, I want to test my ideas. But what can I code? Let's write, and derive something to code from there.\n",
    "\n",
    "## Relationship with other topics\n",
    "Causal models: if we know how to start from unlabeled data and build a map of concepts and relationships, then modeling the causal relationship seems much easier.\n",
    "\n",
    "## Notes\n",
    "[1] \n",
    "\n",
    "In terms of creativity, it could be better to let us mix concepts.\n",
    "\n",
    "## Reading\n",
    "### https://arxiv.org/pdf/1206.5538.pdf\n",
    "perplexity: exponential of the average negative log-likelihood of predicting\n",
    "Bengio and LeCun (2007), \n",
    "What makes a good representation\n",
    "* Multiple explanatory factors\n",
    "* Hierarchical explanatory factors\n",
    "* semi-supervised learning: features to predict p(x) and those to predict p(y|x) could be shared.\n",
    "* natural clustering: information is represented in such a way that for each different class we have a different cluster. It's often the case that a linear interpolation between two different classes yields something with low-probability. \n",
    "\n",
    "http://proceedings.mlr.press/v28/bengio13.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
