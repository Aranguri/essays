{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading notes\n",
    "### https://arxiv.org/pdf/1102.1808.pdf\n",
    "Symbolic reasoning leads to a combinatorial explosion. We don't like them. However, it's difficult to achieve causality with probabilistic models. Another advantage of probabilistic models over the symbolic approach is that the prob. model is continuous, and the distances between elements is meaningful, and if two things are near they are similar. This helps in generalization, because we can assign meanining to dog using a lot of traning cases and transfer some of that meaning to cat even though we didn't see cat being used (the only thing we need is to see a sentence that tells us that cat and dog are similar words.)\n",
    "If we have only a few training cases for a given task, we can use modules trained for other task. In this way, we can create nets that are composition of modules.\n",
    "\n",
    "#### Sentence bracketing\n",
    "There is a problem: how do we concatenate the data. For instance, if we are using words, and modules that take two words and output a higher concept in the same space, then do we go from right to left? Do we build the meaning of the sentences and then concatenate sentence by sentence?\n",
    "One option is to use a module that given a vector in the representation space, it returns us how meaningful it is. Then, we sum all the intermediate terms. Other option is to apply this module only to the output concept, after all the 2 module concepts. \n",
    "Two approaches: beam search (limited memory best-first search) and greedy (take the two with most saliency score) \n",
    "\n",
    "#### Relation to humans\n",
    "Short-term: stack of concepts (either lower or higher concepts)\n",
    "Long-term: parameteres of A and R (A decides how to convert lower concepts into higher. R tells us how good is a given union of concepts.)\n",
    "\n",
    "## https://www.youtube.com/watch?v=KsbftkwZTq4&t=1697s\n",
    "Correlation doesn't care about order. Buying backpacks and computers are correlated (and so are buying computers and backpacks.) But if buying a computer implies buying a backpack, then it may not be the case that buying a backpack implies buying a computer.\n",
    "\n",
    "### Reichenbach's principle\n",
    "How do we link causality with probability? If two events X and Y are statistically dependent, then there should be a shared cause Z (that it could be the same as X or Y)\n",
    "\n",
    "Independence \\iff E[XY] = E[X]E[Y] \\implies having a covariance of zero.\n",
    "\n",
    "Conditional independence: p(x and y|z) = p(x|z)p(y|z)\n",
    "\n",
    "### Functional causal models\n",
    "We have a directed, acyclic graph where each parent is a direct cause of its child node. Each node computes a deterministic function taking as input the parent nodes and noise (the noise stands for unexplained variables.)\n",
    "To make this tractable we assume independence in the functions and the noises.\n",
    "\n",
    "This model fulfills reichenbach's principle, because as the function and noises are independent, the only way for two nodes to be dependent is if they have a shared parent.\n",
    "\n",
    "If we start with some probability for the root nodes, then we can calculate the joint probability of each variable in the graph being some specific value. (We may not start with probabilities for root nodes, we could be starting with noises for the root nodes.) Conversely, given observations from the probability distribution, we can reconstruct the probability distribution, and we can reconstruct the functional causal model.\n",
    "\n",
    "Local causal markov condition: given its parents, a node is independent from its non-descendants.\n",
    "\n",
    "### do-calculus\n",
    "We want to infer what happens to the probability distribution if we assign one value to a variable. (Note this is different from observing that variable taking that specific value.)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
