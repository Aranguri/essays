{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM\n",
    "## Margins\n",
    "https://people.eecs.berkeley.edu/~klein/papers/lagrange-multipliers.pdf\n",
    "We want to know how far are we from the data.\n",
    "Functional margin: a good fit is that if $y = 1$ then we want $h_\\theta(x) >> 0,$ and if $y = 0$ then we want $h_\\theta(x) << 0$\n",
    "Geometric margin: the distance between the points and the boundary\n",
    "\n",
    "### Functional margin \n",
    "\n",
    "Mathematical programming: optimizing with constraints.\n",
    "Linear programming: we have a linear relationship between our variables and a linear constraint. \n",
    "Quadratic programming: we have a quadratic relationship between our variables and a linear constraint.\n",
    "Affine: Linear function + intercept term\n",
    "\n",
    "Lagrange multipliers R. T. Rockarfeller (1970), Convex Analysis, Princeton University Press\n",
    "\n",
    "3:30 wihtout lights\n",
    "\n",
    "https://people.eecs.berkeley.edu/~klein/papers/lagrange-multipliers.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain knearest neighbors, decision trees, SVM, boltzmann machines, autoencoders\n",
    "\n",
    "Shatter: given n datapoints, we have 2^n subsets. We say that a classifier shatters the datapoints if for every subset we have a classifier that separates present points from absent points.\n",
    "\n",
    "VC-dimension: maximum amount of datapoints that we are able to shatter given a function family (examples of function families are hyperplanes.) For instance, we can shatter three points with a line in a 2D space, but we can't shatter four points (think about a square with opposed corners having the same class.)\n",
    "\n",
    "### Metrics\n",
    "Prevalence: positive / examples.\n",
    "\n",
    "Accuracy: (TP + TN) / (TP + TN + FP + FN)\n",
    "Precision: TP / (TP + FP) (here we don't care if we lost some good results ie if FN is large. We care about not letting a FP appear. For instance, in google results, we care about having a good page in the top10 result but it doesn't matter if we forgot about a page)\n",
    "Positive recall (sensitivity): TP / (TP + FN): how many of the positive cases I predicted right.\n",
    "Negative recall (specificity): TN / (TN + FP)\n",
    "?: TN / (TN + FN) (if we are predicting cancer, we might want to avoid at all costs a FN. That is, we don't want a person with cancer to appear as not having it.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1: We use the harmonic mean for the F1 score averaging over the precision and recall because (a) we are using rates {todo: how?} (b) we want both precision and recall to be high. To understand (b), think about labeling everything as a positive example. Then we would have a recall of 100% (there are no negative cases.) But our precision would be low. In particular, if our precision is 0 and recall is 1, the harmonic mean yields 0 (where as )\n",
    "\n",
    "G1: geometric mean between recall and precision\n",
    "\n",
    "#### Log-loss\n",
    "Geometric mean of the gains. We are forcing the model to be confident only when it should be confident. If it's confident and wrong, then the whole logloss is 0 because we have a term that is 0 in the geometric mean.\n",
    "$$\n",
    "Gain := p(x)y + (1 - p(x))(1 - y) \\\\\n",
    "Logloss = (\\Pi_i x_i)^{1/n}\n",
    "$$\n",
    "\n",
    "## Harmonic mean\n",
    "Say we are a snail that goes at 1 mph in uphill and at 2 mph in downhill. If we travel for one hour in downhill and for other hour in uphill, then the average speed is 1.5 mph. Now, what if we want to travel the same distance uphill and downhill? Say we have a 1 mile road. What's the average speed? The average speed should be nearer 1 than 2, because we spend more time at 1 mph than at 2 mph. In uphill, we take 1 hour. In downhill, we take 1 / 2 hours. So, we traveled 1 + 1 = 2 miles in 1 + 1/2 = 1.5 hours. The average mph is 2 / 1.5 = 1.33. The general case is 2 / (1/a + 1/b,) because one over x mph means how much time I take to travel one mile. \n",
    "\n",
    "## Geometric mean"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
