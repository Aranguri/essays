{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLP Seminar 1/11\n",
    "It's useful to skim text. We don't want to give the same importance to everything. \n",
    "LSTM-Jump: the output also has the number of jumps we are going to take.\n",
    "Self-attention: apply attention using the same first and second sentence. This results in an array \n",
    "\n",
    "#### Bengio on youtube\n",
    "Smoothness prior: powered success of deep learning methods. But sometimes we don't have enough data. Thus we need more prior\n",
    "\n",
    "# DLB\n",
    "## Chape 3\n",
    "### Conditional probabilities\n",
    "Computing the probability of event A given that event B happened is different from computing the probability of event A given that we force event B to happen.\n",
    "\n",
    "### Covariance\n",
    "Two dependent variables can result in 0 covariance. \n",
    "\n",
    "### Probability distributions\n",
    "#### Bernoulli\n",
    "$P(x; \\phi) = \\phi^x (1 - \\phi)^(1 - x)$\n",
    "\n",
    "## Chape 4\n",
    "### 4.4 Constrained optimization\n",
    "Say we want to minimize a function f(x) limiting x to having values in a restricted set S.\n",
    "\n",
    "We can traduce every constraint into equations and inequalities. We define \n",
    "$$ L(x, \\lambda, \\alpha) = f(x) + \\sum_i \\lambda_i g^i(x) + \\sum_j \\alpha_j h^j(x)$$\n",
    "where the constraint is that $\\forall i g^i(x) = 0$ and $\\forall j h^j(x) \\leq 0$\n",
    "\n",
    "Then we calculate\n",
    "$$ min_x\\ max_\\lambda\\ max_\\alpha\\ L(x, \\lambda, \\alpha) $$\n",
    "with $\\forall j \\alpha_j \\geq 0$\n",
    "\n",
    "For points that aren't in the set S, the expression above will yield infinite. This happens because either $g^i(x) \\neq 0$ or $h^j(x) > 0$ for some x and some i or j. And then as we have two maxs over the variables $\\lamba$ and $\\alpha,$ we can select infinite for x and that will always make the whole expression infinite.\n",
    "\n",
    "This can also used to maximizing by usign -f(x) instead of just f(x).\n",
    "\n",
    "An inequality constraint is active if h^i(x) = 0. Note that if it's not active, then we know that h^i(x) < 0 for some x. Assuming h^i(x) is continuous, we know that in the neighborhood of h^i(x), all the values of x will satisfy the constraint. Thus, asking for the solution in that neighborhood with and without the constraint won't change the value of the solution. Similarly, that constraint isn't removing solutions for the problem, for there is no x in the neighborhood that has h^i(x) > 0. (fulfilling its \"not active\" name)\n",
    "\n",
    "why it's an xor and a can't be zero always? page 93 be happy\n",
    "\n",
    "## Chape 5\n",
    "### 5.4 \n",
    "#### Consistency\n",
    "Weak|consistency:\n",
    "* $plim_{m\\to\\infty} \\hat\\theta = \\theta$\n",
    "* $\\iff \\forall_\\epsilon P(|\\hat\\theta - \\theta| > \\epsilon) = 0\\ as\\ m \\to \\infty$\n",
    "* $\\forall_{\\hat\\theta} \\hat\\theta$ isn't necessarily equal to $\\theta$\n",
    "\n",
    "Strong consistency:\n",
    "* $P(\\hat\\theta_i = \\theta) = 1\\ as\\ m \\to \\infty$\n",
    "* We can have strong consistency even if $\\hat\\theta_i \\neq \\theta$ for some i.\n",
    "* Example: the amount of food an animal eats per day. At one moment, the animal will die and the amount of food will be zero. Thus, the food the animal eats per day almost sure converges to 0. \n",
    "\n",
    "\n",
    "### 5.7 Supervised learning\n",
    "Probabilistic supervised learning: in most of the cases we want to estimate P(y|x) with P(\\hat y|x; w)\n",
    "Linear regression: P(y|x;w) = N(y;w^Tx, I)$\n",
    "Logistic regression: $P(y|x;w) = \\sigma(y;w^Tx, I)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolution questions\n",
    "* Why 50/50 women, men in humans?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intersting questions\n",
    "P = NP\n",
    "que hay al final del universo\n",
    "\n",
    "# Terms\n",
    "## Limits\n",
    "we say that $lim_{x\\to a} f(x) = l$ if for every $\\epsilon$ you give me, I can select a value r for x s.t. $|f(r) - l| < \\epsilon$\n",
    "\n",
    "For instance, $lim_{x\\to \\infty} f(x) = lim_{x\\to \\infty} (4x - 3) / x = 4.$ Proof. \n",
    "* $f(x) = 4 - 3/x$\n",
    "* $|f(r) - l| < \\epsilon \\iff |4 - 3/r - 4| < \\epsilon \\iff 3/r < \\epsilon \\iff 3 / \\epsilon < r$\n",
    "* Thus, for every $\\epsilon$ you give me, I select r greater than $3 / \\epsilon$ and the condition holds.\n",
    "\n",
    "### plim\n",
    "$plim_{x \\to \\infty} f(x) = L$ means that for any $\\epsilon, P(|f(x) - L| > \\epsilon) \\to 0 \\ as\\ m \\to \\infty$\n",
    "\n",
    "Difference between the plim and lim:\n",
    "* in lim there is a defined r s.t. $|f(r) - l| < \\epsilon.$\n",
    "* in plim that r doesn't exist, for the elements of the sequence come from a random process. What we need to assure is that \n",
    "\n",
    "Alternative understanding: the X_n is a random variable. Thus, for values of X_n in some world, this won't converge to L, but for almost all of them it will."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
