{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre thinking\n",
    "Why could it be that randomly removing a portion of the mlp at each iteration helps in learning?\n",
    "If we train without dropout, the mlp could overfit to the training data. For instance, the train data could have some particularity that you won't find in the test data. Using dropout, at some point we will remove that particularity in the train data. And thus the MLP will learn to focus on more features than just that particularity. \n",
    "\n",
    "Even if train and test data are for the same distribution, as we have finite samples, the test data and train data will be different. They will have different mean and variance.\n",
    "\n",
    "# \n",
    "## Abstract\n",
    "Co-adaptation generalizes the concept in pre-thinking, by saying that we not only shouldn't depend on any particularities of input units, but also shouldn't depend on particularities of any units. Empirically, dropout forces units to do more work on their own, instead of relying on other units.\n",
    "{softweight sharing}\n",
    "\n",
    "## Introduction\n",
    "{Something in the end i didn't understand}\n",
    "\n",
    "## Motivation\n",
    "Evolution: why is there sexual reproduction instead of asexual? One reason could be that we are making genes robust. Specifically, they should be able to work even if some of they partners aren't present (because that will happen after having offspring and after mutations.) Say we mutate a gene G, and say a lot of other genes depended on G. Then we are going to break a lot of the DNA if the genes weren't robust and we mutate G. Thus, the seek for robustness could be an explanation of sexual reproduction. (This concept is related to that of adding noise to a mlp.)\n",
    "\n",
    "Simplicity: in science we almost always seek for simple models. Generally, a simple model will generalize better to test data than a complex one. Why does dropout produce simple models? The smaller the part of the MLP that we used to produce one feature, the lower the chances of one unit being dropped.\n",
    "\n",
    "## Training time\n",
    "Dropout can be combined with a constraint on the norm of the weights, learning rate decay, huge learning rate, and high momentum. The big learning rate and random noise from dropout helps exploring regions not usually explored, while the max-norm prevents the weights from exploding. Finally, the learning rate decays makes it possible to converge. {IDK how momentum is used}\n",
    "\n",
    "## Results\n",
    "Dropout didn't work that well in text compared to the improvement in images. Is it because images are more redundant?\n",
    "\n",
    "## Why does it work?\n",
    "### Sparsity\n",
    "Empirically, this model enforces sparsity. MNIST: For any given input, we usually have most units in zero. And the mean of the activations with dropout is much lower than without dropout.\n",
    "\n",
    "### Effect of dropout rate\n",
    "Part of the generalization error coming from a low dropout rate is that the model doesn't have enough units, not that we are training the mlp with only so many units at a time. \n",
    "\n",
    "### Swweet\n",
    "Experiments showed that when the dataset is small, dropout doesn't give any improvement. This happens because even taking into account the noise, the model can easily overfit to the training data. When the dataset is too large, there is no need for dropout, because the model won't overfit to the training data. There's a sweet spot in-between where dropout is significantly better than no dropout.\n",
    "\n",
    "### Montecarlo\n",
    "Scaling every activation by 1/p isn't the most accurate thing to do, because the units weren't trained to work in that exact regime of the input. In training time, we never considered the case of all the units being active. A more accurate way is to sample k mlps from the full mlp and ensemble them.\n",
    "\n",
    "# Term\n",
    "Max-norm: we limit the weights on a given layer by a constant $||w||_2 < c.$ This implies that no specific weight can be greater than c.\n",
    "costa del \n",
    "## Bayesian neural networks\n",
    "we want to encode a prior on the weights of our nn. \n",
    "We also use bayesian inference to get the values of the hyper|parameters. We thus say that P(W) is proportional to a gaussian. \n",
    "\n",
    "We want to look for the set of weights that are more probable given the dataset.\n",
    "\n",
    "$P(W|D) \\proportional P(W)P(D|W) = P(W)\\prod_{(x, y) \\in D}P(y|x, W)$\n",
    "\n",
    "Note the last step. P(D) is equal to $\\prod_{(x, y) \\in D}P(y|x)$ because the only thing that varies is y (x is given.) Then, we condition on W on both sides.\n",
    "\n",
    "## http://www.cs.ox.ac.uk/people/yarin.gal/website/blog_3d801aa532c1ce.html\n",
    "Uncertainty has at least two purposes. First, it tells the agent what to continue searching. Secondly, it conveys information to the programmer.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
