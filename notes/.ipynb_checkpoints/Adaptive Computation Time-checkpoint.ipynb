{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Say we have a rnn and we want it to decide on its own when to stop.\n",
    "\n",
    "One approach is to consider the last layer at each timestep. We can do this by seeing whether a halting unit passed some threshold. Also, we can make it stochastic and define the probability of a halt using a binomial distribution given by the value of the halting unit. Another option is to measure its security on the output for the task. We can do this by executing the neural network multiple times and observing the variance of the result (low variance means security.) \n",
    "\n",
    "Other idea is to interpret the halting unit at one timestep as how much we should consider the output at that timestep. This generalizes the previous idea because the nn can output always 0 in the halting unit and then output 1.\n",
    "\n",
    "{how do self-delimiting neural networks train the halting unit?}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that for RNNs we have two dimensions. First, we have the time dimension to process long sequences applying the same LSTM transformation. Second: we have the processing dimension where we may want multiple layers to process one input (this is super common in feedforward nns.)\n",
    "\n",
    "<img src=\"https://slideplayer.com/slide/14889574/91/images/7/Deep+RNN+RNNs+with+multiple+hidden+layers+y1+y2+y3+y4+y5+y6+x1+x2+x3.jpg\" style=\"width: 200px\">\n",
    "\n",
    "The modification from ACT is to make variable the length of the processing dimension.\n",
    "\n",
    "At each processing step n we produce three vectors:\n",
    "$$y_t^n, s_t^n, h_t^n$$\n",
    "\n",
    "$h_t^n$ stores the halting variable. If $\\sum_n^K h_t^n > 1 - \\epsilon,$ then we halt execution. Then, we copy $h_t^n$ into $p_t^n$ for each n except for n = K (the last value of n.) To make $\\sum_n^K h_t^n$ equals to $1,$ we set $p_t^K = 1 - \\sum_n^{K-1} h_t^n$\n",
    "\n",
    "Then, the output vector in the processing dimension is as follows\n",
    "$$s_t = \\sum_n^K p_t^n s_t^n$$\n",
    "$$y_t = \\sum_n^K p_t^n y_t^n$$\n",
    "\n",
    "### 2.1\n",
    "We'd want to penalize using too much time to think. Thus, we define the ponder cost at timestep t as the sum of the processing steps used (N(t)) and the remaining budget after the last processing step (R(t))\n",
    "$$p_t = N(t) + R(t)$$\n",
    "$$p = \\sum_t p_t$$\n",
    "$$\\hat L = L + \\tau p$$\n",
    "I'm not sure why we minimize R(t) if our goal is to minimize N(t). One explanation could be that by minimizing R(t) we are making each processing step bigger and thus indirectly minimizing N(t). \n",
    "\n",
    "### 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding this limit in its most general\n",
    "form would be equivalent to determining the Kolmogorov complexity of the data (and hence solving\n",
    "the halting problem) [21]. why?\n",
    "Hogwild!\n",
    "\n",
    "## 3\n",
    "Penalizing less the ponder time allows for more ponder time, which in turn allows a lower sequence error rate at the cost of more processing.\n",
    "\n",
    "# Terms\n",
    "Sequence error rate: times there was at least one error in the sequence.\n",
    "Mean-field vector: weighted average"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
