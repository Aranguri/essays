{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea: create an encoder for every type of media (text, images, audio) and also a decoder for every type. Train every combination of encoder-decoders with a fixed representaiton in the middle, so as to force them to find a way to represent different types of data in the same way. \n",
    "### Idea: in the LSTM cell we may want the vectors i and f to sum to one. \n",
    "\n",
    "\n",
    "try a module that receives two word embeddings and returns one word embedding\n",
    "Try word embeddings with one/two/three number. It should have enough memory (2^32, I think)\n",
    "In bottou 2011 we use the same space for individual words and for higher concepts. Why don't we use two/three different spaces.\n",
    "\n",
    "### Bias\n",
    "It's interesting that gradient descent doesn't take into account the value of the biases (at least it doesn't do that directly.) Are we wasting information by not using that value?\n",
    "\n",
    "\n",
    "\n",
    "Elements: ensembles (similar to dropout (similar to relu working))\n",
    "\n",
    "Try making more robust word embedding by adding noise. Do we want redundancy?\n",
    "\n",
    "What happens to the weights of the net if we train it different times? What's the space of nns that solve some problem?\n",
    "\n",
    "Elements: way to measure similarity between x and y (attention, dot product)\n",
    "\n",
    "What's the training objective in humans? What is a good traninig objective for somethign that generalizes?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
