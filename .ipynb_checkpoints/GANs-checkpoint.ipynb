{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generative types: (a) model the entire distribution or (b) be able to produce samples.\n",
    "\n",
    "We want to maximize the probability of the data:\n",
    "\\theta = argmax_\\theta p(x;\\theta) = argmax\\theta \\Pi_i p(x_i;\\theta)\n",
    "this is equivalent to minimizing the KL divergence between p_{model} and p_{data} (or p hat)\n",
    "\n",
    "Deep generative models classes\n",
    "* Model explicitly the probability distribution of the data\n",
    "    * Approximations\n",
    "        * Monte Carlo Markov Chains\n",
    "        * Variational approximations\n",
    "    * Exact\n",
    "        * \n",
    "* Don't model explicitly\n",
    "    * Markov Chains\n",
    "    * Direct?: GAN\n",
    "\n",
    " Finn et al. (2016b)\n",
    " \n",
    "Inverse reinforcement learning: in rl we try to get the optimal policy given a reward function. in inverse rl we assume the agent is using the optimal policy, so we try to deduce which reward function has that behavior as optimal.\n",
    "Semi-supervised learning: using both labeled and unlabeled examples.\n",
    "Multi-modal: when one input has several different correct answers. For example, if we're predicting the next word given 10 past words, we don't want to force the model to one particular answer.\n",
    "\n",
    "### Approximations\n",
    "#### Monte Carlo Markov Chains\n",
    "Markov Chain: a random walk in a directed graph with probabilities for each edge.\n",
    "\n",
    "STationary distribution: if you make a lot of steps in the graph, where you end doesn't relate to where you started.\n",
    "\n",
    "Connected graph: every nodes connects to at least one node (without considering directions)\n",
    "\n",
    "Strongly connected graph: every node has at least one outgoing connection (in a directed graph) \n",
    "\n",
    "#### Variational approximations\n",
    "We propose a family of functions, and we use optimization to find the parameters for the function that better explain the data.\n",
    "\n",
    "### ICA (ica ica caballitoi)\n",
    "Consider the problem of n microphones all over a room with n people talking. Assume that what we listen in each microphone is a linear combination of the people's voices. Formally, \n",
    "$x = As$\n",
    "s is what people is saying and x is what we hear in the microphones.\n",
    "We want to estimate A^{-1}. Thus, we define the log-likelihood in terms of A^{-1}, and then we maximize it.\n",
    "\n",
    "We can recover s up to two things\n",
    "* If we swap two rows in W and two elements in s, we get the same x. So ICA doesn't detects the source up to a rotation.\n",
    "* x has to be non-gaussian. Otherwise, as a gaussian distribution is rotationally symmetric, we are going to receive the same distribution for x even if we multiply W by a rotation matrix. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
