{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generative types: (a) model the entire distribution or (b) be able to produce samples.\n",
    "\n",
    "We want to maximize the probability of the data:\n",
    "\\theta = argmax_\\theta p(x;\\theta) = argmax\\theta \\Pi_i p(x_i;\\theta)\n",
    "this is equivalent to minimizing the KL divergence between p_{model} and p_{data} (or p hat)\n",
    "\n",
    "Deep generative models classes\n",
    "* Model explicitly the probability distribution of the data\n",
    "    * Approximations\n",
    "        * Monte Carlo Markov Chains\n",
    "        * Variational approximations\n",
    "    * Exact\n",
    "        * \n",
    "* Don't model explicitly\n",
    "    * Markov Chains\n",
    "    * Direct?: GAN\n",
    "\n",
    " Finn et al. (2016b)\n",
    " \n",
    "Inverse reinforcement learning: in rl we try to get the optimal policy given a reward function. in inverse rl we assume the agent is using the optimal policy, so we try to deduce which reward function has that behavior as optimal.\n",
    "Semi-supervised learning: using both labeled and unlabeled examples.\n",
    "Multi-modal: when one input has several different correct answers. For example, if we're predicting the next word given 10 past words, we don't want to force the model to one particular answer.\n",
    "\n",
    "### Approximations\n",
    "#### Monte Carlo Markov Chains\n",
    "Markov Chain: a random walk in a directed graph with probabilities for each edge.\n",
    "\n",
    "STationary distribution: if you make a lot of steps in the graph, where you end doesn't relate to where you started.\n",
    "\n",
    "Connected graph: every nodes connects to at least one node (without considering directions)\n",
    "\n",
    "Strongly connected graph: every node has at least one outgoing connection (in a directed graph) \n",
    "\n",
    "#### Variational approximations\n",
    "We propose a family of functions, and we use optimization to find the parameters for the function that better explain the data.\n",
    "\n",
    "### ICA (ica ica caballitoi)\n",
    "#### Linmitations\n",
    "Consider the problem of n microphones all over a room with n people talking. Assume that what we listen in each microphone is a linear combination of the people's voices. Formally, \n",
    "$x = As$\n",
    "s is what people is saying and x is what we hear in the microphones.\n",
    "We want to estimate A^{-1}. Thus, we define the log-likelihood in terms of A^{-1}, and then we maximize it.\n",
    "\n",
    "We can recover s up to two things\n",
    "* If we swap two rows in W and two elements in s, we get the same x. So ICA doesn't detects the source up to a rotation.\n",
    "* x has to be non-gaussian. Otherwise, as a gaussian distribution is rotationally symmetric, we are going to receive the same distribution for x even if we multiply W by a rotation matrix. \n",
    "\n",
    "#### Exchanging PDFs\n",
    "Say we have a n-dimensional vector s that has a uniform probability density function over the range [0, 1]. Let's define C_1 as the n-dimensional hypercube. C_1 represents the PDF of s in the following way p_s(s) = 1{s \\in C_1}. Now, let's transform C_1 by A = 2I. In this way, we get C_2 = {As | s \\in C_1}. C_2 is no more a PDF, for it integrates to |A| = 2^n instead of 1. But we want to continue having a PDF, so we divide by the determinant. Formally, p_x(x) = 1{x \\in C_2}\\|A| = 1{x \\in C_2}|A^{-1}| = 1{x \\in C_2}|W|. Now, $x \\in C_2 \\iff A^{-1}x \\in C_2.$ Thus, $p_x(x) = 1\\{Wx \\in C_1\\}|W| = p_s(Wx)|W|.$ \n",
    "\n",
    "Think about it geometrically. The area of the PDF has to remain constant (equal to one.) Say we start with a valid PDF p. Now, if we extend/shrink the range of p, we need to shrink/extend the probabilities assigned to every number by the same amount as the range was changed.\n",
    "\n",
    "#### De algorithm\n",
    "We know that every speaker $s_i$ is independent, so \n",
    "$$\n",
    "p(s) = \\prod_i p_s(s_i) \\\\\n",
    "$$\n",
    "We know that s is a linear combination of x (what we hear in the microphones)\n",
    "$$\n",
    "s = Wx \\\\ \n",
    "p(Wx) = \\prod_i p_s(w^T_ix) \\\\\n",
    "$$\n",
    "We know the relationship between p_x and p_s\n",
    "$$\n",
    "p_x(x) = p_s(Wx)|W| \\\\\n",
    "p_x(x) = \\prod_i p_s(w^T_ix)|W|\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that we know the values of x. So we are looking for the value of w that make the training data the most probable. The training data is everything we know, and making the data most probable means we are picking the w that is most probable given the data. \n",
    "\n",
    "But we can't take derivatives on p_x(x), because there's no parameter to tune and we don't know about the pdf p_x. That's why we derived an expression for p_x(x) that uses w, x, and p_s. Now, we know x, we are going to find w, but we need to also know p_s. We assume p_s is the derivative of a sigmoid ($\\iff$ the cdf of p_s is sigmoid.) \n",
    "\n",
    "Then, we calculate the log-likelihood of $\\prod_i p_s(w^T_ix)|W|,$ yielding (for one training example)\n",
    "$$\n",
    "l_j(W) = \\sum_i log(p_s(w^T_ix)) + log|W|\n",
    "$$\n",
    "Then, we take derivatives to get\n",
    "$$\n",
    "\\nabla_w l_j = \\sum_i (1 - 2g(w^T_ix))x^T + log|W|\n",
    "$$\n",
    "And then we use gradient ascent to get the w that maximizes the likelihood of p_x(x) (running over all the traninig examples)\n",
    "{todo: do mle work for non-convex}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
